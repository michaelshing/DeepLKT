{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"%load_ext autoreload\n%autoreload 2"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/home/sudeep/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/sudeep/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/sudeep/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/sudeep/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/sudeep/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/sudeep/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"}],"source":"import torch\nimport torch.nn as nn\nfrom deeplkt.datasets.dataset import *\nfrom torch.utils.data import DataLoader\nfrom deeplkt.models.pure_lkt import PureLKTNet\nfrom deeplkt.models.lkt_alexsobel import LKTAlexSobelNet\n\nfrom deeplkt.models.lkt_vggsobel import LKTVGGSobelNet\nfrom deeplkt.models.lkt_vggimproved import LKTVGGImproved\nfrom deeplkt.models.base_model import BaseModel\n\nfrom deeplkt.utils.util import dotdict\nfrom deeplkt.tracker.lkt_tracker import LKTTracker\nfrom deeplkt.config import *"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"True\n"}],"source":"\nuse_cuda = torch.cuda.is_available()\nprint(use_cuda)\ndevice = torch.device(\"cuda\") if use_cuda else torch.device(\"cpu\")\n\n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Parsing VOT dataset...\nVOT dataset parsing done.\nTotal number of annotations in VOT dataset = 10651\nTotal number of videos in VOT dataset = 26\nParsing ALOV dataset...\nALOV dataset parsing done.\nTotal number of videos in ALOV dataset = 307\nTotal number of annotations in ALOV dataset = 15877\n"}],"source":"#!/usr/bin/env python\n# coding: utf-8\n\n\n# from pytorch_practise import *\n\n\nvot_root_dir = '../../data/VOT/'\nalov_root_dir = '../../data/ALOV/'\n\n\nvot = VotDataset(os.path.join(vot_root_dir,\n                       'VOT_images/'),\n                 os.path.join(vot_root_dir,\n                       'VOT_ann/'),\n                 os.path.join(vot_root_dir,\n                       'VOT_results/'), \n                 device)\n\nalov = AlovDataset(os.path.join(alov_root_dir,\n                       'ALOV_images/'),\n                   os.path.join(alov_root_dir,\n                       'ALOV_ann/'),\n                   os.path.join(alov_root_dir,\n                       'ALOV_results/'), \n                       device)\n\n\n# train_loader = DataLoader(alov, batch_size=1, shuffle=False)\n\n"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"True"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"import cv2\nimg = vot.show_image_with_gt(0,1)\ncv2.imwrite(\"vot_0_1.jpeg\", img)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"from deeplkt.config import *\nparams = dotdict({\n    'mode' : MODE,\n    'max_iterations' : MAX_LK_ITERATIONS,\n    'epsilon' : EPSILON,\n    'info': \"VGGSobel LKT\"\n})\n# lr = 0.0005\n# momentum = 0.5\n\n\nnet = LKTVGGImproved(device, params)\ntracker = LKTTracker(net)\ntrain_params = dotdict({\n    'batch_size' : BATCH_SIZE,\n    'val_split' : VALIDATION_SPLIT,\n    'train_examples':TRAIN_EXAMPLES,\n    'shuffle_train': SHUFFLE_TRAIN,\n    'random_seed': RANDOM_SEED,\n    'lr': LR,\n    'momentum': MOMENTUM,\n    'l2': L2\n\n})\n\nmodel = BaseModel(tracker, 'checkpoint', 'logs', train_params)\n\n"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([1, 3000, 1, 3, 3])\ntorch.Size([1, 3000, 1, 3, 3])\n"}],"source":"for p in net.parameters():\n    if(p.requires_grad):\n        print(p.shape)\n"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":"54000"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"# print(net)\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\ncount_parameters(net)\n"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"checkpoint/VGGSobel LKT/45-checkpoint.pth\nEvaluating dataset for video  1\nTime taken =  87.22712922096252\nMean iou =  0.014401258432961176\n"},{"data":{"text/plain":"0.014401258432961176"},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":"model.load_checkpoint(45)\nmodel.eval_model(vot, 1)\n"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[[[[-9.9393e-01, -3.6622e-02,  9.2164e-01],\n           [-1.9538e+00, -3.7756e-02,  1.8793e+00],\n           [-9.9468e-01, -3.7436e-02,  9.2091e-01]]],\n\n\n         [[[-1.0711e+00, -1.1318e-01,  8.4654e-01],\n           [-2.0285e+00, -1.1169e-01,  1.8068e+00],\n           [-1.0677e+00, -1.0959e-01,  8.5013e-01]]],\n\n\n         [[[-9.4848e-01,  9.3161e-03,  9.6728e-01],\n           [-1.9078e+00,  8.7496e-03,  1.9255e+00],\n           [-9.5028e-01,  7.5542e-03,  9.6546e-01]]],\n\n\n         ...,\n\n\n         [[[-9.5783e-01,  8.8215e-04,  9.5971e-01],\n           [-1.9166e+00,  8.0595e-04,  1.9184e+00],\n           [-9.5787e-01,  8.4408e-04,  9.5967e-01]]],\n\n\n         [[[-9.6409e-01, -5.4916e-03,  9.5332e-01],\n           [-1.9228e+00, -5.4966e-03,  1.9121e+00],\n           [-9.6431e-01, -5.7100e-03,  9.5311e-01]]],\n\n\n         [[[-9.5881e-01, -2.0641e-05,  9.5877e-01],\n           [-1.9175e+00,  5.2860e-06,  1.9175e+00],\n           [-9.5881e-01, -1.4590e-05,  9.5877e-01]]]]],\n       device='cuda:0', dtype=torch.float64)\ntensor([[[[[-9.9393e-01, -3.6622e-02,  9.2164e-01],\n           [-1.9538e+00, -3.7756e-02,  1.8793e+00],\n           [-9.9468e-01, -3.7436e-02,  9.2091e-01]]],\n\n\n         [[[-1.0711e+00, -1.1318e-01,  8.4654e-01],\n           [-2.0285e+00, -1.1169e-01,  1.8068e+00],\n           [-1.0677e+00, -1.0959e-01,  8.5013e-01]]],\n\n\n         [[[-9.4848e-01,  9.3161e-03,  9.6728e-01],\n           [-1.9078e+00,  8.7496e-03,  1.9255e+00],\n           [-9.5028e-01,  7.5542e-03,  9.6546e-01]]],\n\n\n         ...,\n\n\n         [[[-9.5783e-01,  8.8215e-04,  9.5971e-01],\n           [-1.9166e+00,  8.0595e-04,  1.9184e+00],\n           [-9.5787e-01,  8.4408e-04,  9.5967e-01]]],\n\n\n         [[[-9.6409e-01, -5.4916e-03,  9.5332e-01],\n           [-1.9228e+00, -5.4966e-03,  1.9121e+00],\n           [-9.6431e-01, -5.7100e-03,  9.5311e-01]]],\n\n\n         [[[-9.5881e-01, -2.0641e-05,  9.5877e-01],\n           [-1.9175e+00,  5.2860e-06,  1.9175e+00],\n           [-9.5881e-01, -1.4590e-05,  9.5877e-01]]]]],\n       device='cuda:0', dtype=torch.float64)\n"}],"source":"mp = model.nn.model.state_dict()\nfor i, k in enumerate(mp):\n    if(i < 4 and i %2 == 0):\n        print(mp[k])"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"LKTVGGSobelNet(\n  (pad): ReflectionPad2d((1, 1, 1, 1))\n  (vgg): VGGImproved(\n    (pad): ReflectionPad2d((1, 1, 1, 1))\n    (vgg): VGG(\n      (features): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU(inplace)\n        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU(inplace)\n        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (6): ReLU(inplace)\n        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (8): ReLU(inplace)\n        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (11): ReLU(inplace)\n        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (13): ReLU(inplace)\n        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (15): ReLU(inplace)\n        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (18): ReLU(inplace)\n        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (20): ReLU(inplace)\n        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (22): ReLU(inplace)\n        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (25): ReLU(inplace)\n        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (27): ReLU(inplace)\n        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (29): ReLU(inplace)\n        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      )\n      (classifier): Sequential(\n        (0): Linear(in_features=25088, out_features=4096, bias=True)\n        (1): ReLU(inplace)\n        (2): Dropout(p=0.5)\n        (3): Linear(in_features=4096, out_features=4096, bias=True)\n        (4): ReLU(inplace)\n        (5): Dropout(p=0.5)\n        (6): Linear(in_features=4096, out_features=1000, bias=True)\n      )\n    )\n    (soft): Softmax()\n  )\n)\n"}],"source":"\nmodel.load_checkpoint(74)\n"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Evaluating dataset for video  2\n(1, 8)\n(1, 240, 320, 3)\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\ntorch.Size([1, 3, 127, 127])\nTime taken =  18.056737899780273\nMean iou =  0.07419289474269634\n18.88970422744751\n"}],"source":"from time import time\nstart_t = time()\nmodel.eval_model(vot, 2)\nend_t = time()\nprint(end_t - start_t)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}